
# ü§ñ Guide d'installation d'Ollama

> Pour activer le chat IA local dans Help-Desk

## Qu'est-ce qu'Ollama ?

**Ollama** est un logiciel qui permet de faire tourner des mod√®les d'IA (comme ChatGPT) **directement sur ton ordinateur**, sans envoyer tes donn√©es sur Internet.

---

## Installation

### Windows

#### 1Ô∏è‚É£ T√©l√©charger Ollama

Va sur [ollama.ai](https://ollama.ai/download) et t√©l√©charge l'installeur Windows.

#### 2Ô∏è‚É£ Installer

Double-clique sur le fichier `.exe` et suis les instructions.

#### 3Ô∏è‚É£ V√©rifier l'installation

Ouvre un terminal et tape :
```bash
ollama --version
```
### Linux

#### 1Ô∏è‚É£ Installer avec le script officiel

curl -fsSL https://ollama.ai/install.sh | sh

#### 2Ô∏è‚É£ V√©rifier

ollama --version

### macOS

#### 1Ô∏è‚É£ T√©l√©charger Ollama
Va sur [ollama.ai](https://ollama.com/download/mac) 

#### 2Ô∏è‚É£ Installer

Glisser dans Applications

#### 3Ô∏è‚É£ V√©rifier

ollama --version

## T√©l√©charger le mod√®le IA
    Help-Desk utilise le mod√®le llama3.1:8b (recommand√© pour un bon √©quilibre performance/qualit√©).

#### 1Ô∏è‚É£ T√©l√©charger le mod√®le

ollama pull llama3.1:8b
    Patience : Le t√©l√©chargement peut prendre 5-10 minutes (environ 4,7 Go).

#### 2Ô∏è‚É£ V√©rifier que le mod√®le est pr√™t

ollama list

## Utilisation avec Help-Desk

### Windows/Linux/Mac :

ollama serve

### Lancer Help-Desk

streamlit run main.py

    Va dans "Chat IA" ‚Üí √áa devrait fonctionner !

## D√©pannage

1. Erreur "Connection refused"

Probl√®me : Ollama n'est pas d√©marr√©.

Solution :
```bash
ollama serve
```
Laisse ce terminal ouvert.

2. Le mod√®le ne r√©pond pas

Probl√®me : Le mod√®le n'est pas t√©l√©charg√©.

Solution :
```bash
ollama pull llama3.1:8b
```

3. Performances lentes

Probl√®me : Ton PC est trop ancien pour faire tourner l'IA localement.

Solutions :

    Utilise un mod√®le plus l√©ger :
```bash
ollama pull llama3.1:3b
```

## Configuration avanc√©e

### Changer de mod√®le
Pour utiliser un autre mod√®le (ex: mistral), modifie utils/ollama_client.py :

```bash
MODEL = "mistral"  # Au lieu de llama3.1:8b
```

### Mod√®les disponibles
Liste compl√®te : ollama.ai/library
Recommandations :

    llama3.1:8b (d√©faut, bon √©quilibre)
    llama3.1:3b (plus rapide, moins pr√©cis)
    mistral (alternatif, tr√®s performant)

## D√©sinstaller Ollama

### Windows
Panneau de configuration ‚Üí D√©sinstaller un programme ‚Üí Ollama

### Linux 
```bash
sudo systemctl stop ollama
sudo systemctl disable ollama
sudo rm /usr/local/bin/ollama
```

### macOS
Glisse Ollama depuis Applications vers la Corbeille.

## Ressources

    Site officiel : ollama.ai
    Documentation : github.com/ollama/ollama







